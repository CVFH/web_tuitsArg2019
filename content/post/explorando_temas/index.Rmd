---
title: "Explorando Temas"
author: "Carolina Franco"
date: "2021-01-26"
tags: ["análisis", "temas"]
weight: 8
---

Hemos hecho un largo recorrido. Teníamos en mente un interrogante amplio: **¿Cuánto y qué expresaron los candidatos a las elecciones de 2019 en la Argentina?**

El necesario punto de partida de nuestra respuesta exploratoria ha sido la presentación del "esqueleto" y "la carne": las [funciones](../preparacion_funciones/) y los [datos](../preparacion_datos/) que pusieron en movimiento a nuestro análisis. 

Hasta aquí, este procedió en tres etapas. Primero, [exploramos visualmente las relaciones entre el desempeño de los candidatos en la red social y en las urnas](../explorando_popularidad/). En segundo lugar, hicimos lo propio con [las palabras](../explorando_palabras/): qué tan elocuentes fueron y qué términos prefirieron nuestros candidatos. [En el tercer y anterior post](../explorando_relaciones/), retuvimos un "tipo" de término muy especial: las _menciones_, con base en el cual diagramamos algunos de los vínculos o comunidades que habilita el uso de _Twitter_. 

En esta última muestra de análisis intentaremos profundizar en la mirada global acerca de lo dicho por los candidatos, tratando de asir más similitudes y diferencias entre sus discursos. Nuestra pregunta es sencilla: **¿de qué hablan los candidatos?** ¿a qué temas se refieren?

Responderla es un poco más difícil. En lo que sigue ensayaremos, con diferentes grados de éxito, tres abordajes posibles.

1. En primer lugar, rastreamos términos cuya pertenencia adscribimos, manualmente (_arbitrariamente_), a ciertos "temas".
2. En segundo lugar, utilizamos la técnica de _topic modelling_.
3. Finalmente, comentaremos brevemente los intentos infructuosos por aplicar el _Análisis de Componentes Principales_ a nuestros datos.

¡a por ello!

### Preparando la mesa de trabajo 

#### Apertura de librerías

Una vez más activamos funciones propias y ajenas. Novedades para la ocasión: el paquete `ggforce` para graficar "sets paralelos" y, muy importante, `tm` y `topicmodels`, que nos permitirán ejecutar parte del análisis más adelante.

```{r librerias, warning= FALSE, message= FALSE}
#paquetes

library(readxl) # lectura de archivos excel
library(ggplot2) # gráficos
library(ggthemes) # temas de graficos
library(reshape2)
library(tm) # para DocumentTermMatrixs
library(topicmodels) # para modelado de topicos
library(ggbiplot) # para graficar PCA
library(igraph) # para grafos
library(pander)
library(tidyverse)
library(ggforce) # para graficos de sets paralelos
library(twitterwidget) # para incluir tuits en Rmd 
library(gt) # para formateo de tablas

#propias

source("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/Modules/tuitsCandidatos.R", encoding = "UTF-8") # para trabajar con bases de tuits
```

#### Importación de datos

Y vengan los datos. Más adelante incorporaremos otra base de interés. Por ahora, traemos a `joined_candidatos`, el nombre que hemos puesto al agregado de los tuits emitidos por los contendientes en la carrera electoral.

```{r datos, warning= FALSE, message= FALSE}
# Temas por palabras 

# invocamos al script con las funciones correspondientes

source("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/preparacion_datos_tuits.R", encoding = "UTF-8")

# traemos datos

datos_base <- traerDatosTuits("base")
joined_candidatos <- traerDatosTuits("tot") %>% 
  left_join(datos_base)
```

Ahora sí, pasemos a consignar nuestros intentos.

## 1: Rastreo manual de temas por palabras

Una primera forma de rastrear _acerca de qué_ se han pronunciado nuestros emisores es identificar "palabras clave" que podemos presumir asociadas a un determinado asunto. 

Ahora bien, determinar qué palabras corresponden a qué temas no es tarea fácil. Trazar límites entre cuestiones implica encorsetar un objeto fluido, creativo y ambiguo como el lenguaje en categorías estancas y, en nuestro caso, abiertamente arbitrarias, ya que para este ejercicio hemos optado por definirlas _a priori_.

En breve, utilizaremos una "lista de palabras" que consideramos usualmente asociadas a "temas", y veremos qué tanto coinciden o no los tuits de los candidatos con las palabras de esta lista y, por ende, con los asuntos o problemáticas previamente identificadas.

> Sin dudas, existen técnicas menos arbitrarias para diagramar estos "listados de palabras". Se podría, por caso rastrearlos a partir de una muestra más reducida de nuestros propios datos, o, inversamente, en una base más amplia, como sean notas de prensa. A nuestros fines exploratorios, baste la estrategia elegida. 

A continuación, importamos un _data frame_ cuyas variables son los "temas" que vamos a buscar, y cuyas observaciones constituyen los términos que asociamos a cada tema[^1]. Inspeccionamos cómo está estructurada más abajo. 

```{r datos_palabras, warning= FALSE, message= FALSE}
# importamos la base

temas_palabras <- read_xlsx("temas_palabras.xlsx")

# inspeccionamos la base

str(temas_palabras)
```

Para identificar en qué medida los términos previamente identificados "aparecen" en los tuits, procederemos a descomponer estos últimos en palabras. Nuestro objetivo será, en el próximo paso, contabilizar las "coincidencias".

```{r tokenizado, warning= FALSE, message= FALSE}
candidatos_tokenizadas <- joined_candidatos %>% 
tokenizarTextoTuits() %>% 
left_join(datos_base)
```

Tenemos entonces dos conjuntos de datos:

1. Temas asociados a listas de palabras.
2. Una lista de palabras asociadas a tuits emitidos por los candidatos.

En el código a continuación, procedemos de manera iterativa para rastrear las coincidencias entre ambos. 

Para cada columna (cada tema) en `tema_palabras`, recorremos la lista de palabras extraída de la base con tuits de los candidatos. Durante cada vuelta de nuestro recorrido, añadiremos una nueva variable a esta última: sus filas contendrán un "1" si la palabra del tuit pertenece al asunto en cuestión, y un "0" si no hay coincidencias. 

> Más abajo, una vez finalizadas todas las vueltas, y por ende rastreadas todas las posibles coincidencias, modificamos la disposición de nuestros datos con `pivot_longer`. De este modo, nuestra base resultante constituye un data frame cuyas observaciones (filas) son las combinatorias palabras-tema. Esta estructura nos resultó más adecuada al análisis posterior.

```{r temas iteracion, warning= FALSE, message= FALSE}

temas_palabras_match_tokens <- candidatos_tokenizadas %>% 
  limpiarTokens(palabras_web = TRUE, hashtags = TRUE, mentions = TRUE) %>% 
  select(screen_name, tweet_id, tokens)

# calculando coincidencias

for (columna in 1:ncol(temas_palabras)) {
  
  testear_coincidencias <- na.omit(as.data.frame(temas_palabras[columna])) %>% 
    rename( palabras = colnames(temas_palabras[columna]) )
  
  new <- ifelse( temas_palabras_match_tokens$tokens %in% testear_coincidencias$palabras, 
                 "1", 
                 "0")
  
  temas_palabras_match_tokens[ , ncol(temas_palabras_match_tokens) + 1] <- new                  # Append new column
  
  colnames(temas_palabras_match_tokens)[ncol(temas_palabras_match_tokens)] <- colnames(temas_palabras[columna])  # Rename column name
}

# hacemos matriz "long"

temas_palabras_match_tokens_long <- temas_palabras_match_tokens %>%
  pivot_longer(!c(screen_name, tweet_id, tokens), 
               # una fila para cada combinación tuit/token/tema
               names_to = "temas", values_to = "count") %>%  
  # con una columna (count) que indica si está presente el tema en ese token-tuit
  filter(count==1) 
# nos quedamos sólo con los tokens asignados a un tema

```

Ahora sí, estamos en condiciones de inspeccionar los resultados.

Rastrearemos primero cuántas coincidencias hubo por tuit. Recordemos que habíamos descompuesto los tuits en palabras, y buscado matches entre estas últimas y la lista definida previamente. Entonces, en un tuit puede haber desde ninguna o una única palabra asociada a un tema, hasta varias.

```{r primera_inspeccion_temas, warning= FALSE, message= FALSE}
# inspeccionando resultados / primera aproximacion

# cantidad de coincidencias por tuit

coincidencias_tweets <- temas_palabras_match_tokens_long %>% 
  group_by(tweet_id) %>% 
  dplyr::summarise(cantidad_coincidencias = sum(as.integer(count))) %>% 
  left_join(joined_candidatos)

# cantidad de coincidencias por tema por tuit

ncoincidencias_tema_tweets <- left_join(joined_candidatos %>% 
                                          filter(Campaña == 1 ),
                                        temas_palabras_match_tokens_long %>% 
                                          dplyr::count(tweet_id, temas) 
                                        ) %>% 
  select(tweet_id, screen_name, text, temas, n) %>% 
  dplyr::rename(coincidencias_tema_tuit = "n")
```

Veamos un ejemplo de un tuit con múltiples coincidencias.

```{r ejemplo1, warning= FALSE, message= FALSE}
# ejemplo: tuit con muchas coincidencias sobre un mismo tema

ejemplo1 <- ncoincidencias_tema_tweets %>%  
  arrange(desc(coincidencias_tema_tuit)) %>% 
  head(1)

twitterwidget(as.character(ejemplo1$tweet_id))

```

Con estos datos ya podemos explorar los temas más populares. Es decir, los temas que tuvieron más cantidad de coincidencias.

> Atención: recordemos que se trata de una cuenta abiertamente arbitraria. La cantidad de palabras que asociamos a cada tema es variable. Además, nada dice que todas sean igual de importantes en un sentido sustantivo. De todos modos, no está de más el ejercicio.

```{r temas_populares, warning= FALSE, message= FALSE}
# temas mas populares

temas_populares <- fct_count(ncoincidencias_tema_tweets$temas) %>% 
  arrange(desc(n)) %>% 
  dplyr::rename(Tema = "f", "Cantidad de tuits" = "n") %>% 
  na.omit()

head(temas_populares, 10) %>% 
  gt::gt() %>% 
  gt::tab_header(
    title = "Temas más populares entre los candidatos",
    subtitle = "(conteo manual)") %>% 
  gt::tab_style(
    style=  cell_fill(color = "#00BFFF", alpha = 0.5),
    locations = cells_title(groups = c("title", "subtitle"))) %>% 
  gt::tab_style(
    style=  cell_fill(color = "#E9EDF1", alpha = 0.5),
    locations = cells_body()) %>% 
  gt::tab_style(
    style= cell_text(
      color = "#050505",
      align = "center",
      v_align = "middle",
      weight = "lighter"),
    locations = cells_body())
```

Sigamos con algunos cálculos más relevantes a nuestros fines. Dada la anterior advertencia, y el origen de nuestros datos, encontramos más provechoso trabajar con los _tuits_ antes que con las palabras dispersas.

Reuniremos otra vez las palabras en tutis, entonces, e identificaremos a qué temas fue asignado cada tuit con base en sus palabras. 

```{r calculo_temas_tuit, warning= FALSE, message= FALSE}
# cantidad de temas por tuit

ntemas_tweets <- temas_palabras_match_tokens_long %>% 
  dplyr::count(tweet_id, temas) %>% 
  dplyr::mutate( tweet_id = as.factor(tweet_id))

# unimos con base de datos

cantidad_temas_tuit <- left_join(joined_candidatos %>% 
                                   filter(Campaña == 1 ) %>% 
                                   dplyr::mutate(tweet_id = as.factor(tweet_id)),
                                 fct_count(ntemas_tweets$tweet_id) %>% 
                                   dplyr::rename(tweet_id = "f")) %>% 
  select(tweet_id, screen_name, text, n) %>%
  mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .))) %>% 
  dplyr::rename(cantidad_temas_tuit = "n")
```

Cabe la posibilidad de que un único tuit haya sido asignado a más de un tema. Calculamos esto en el último tramo del código recién expuesto. Abajo exponemos el tuit asignado a más temas de nuestra base de datos. 

```{r ejemplo2, warning= FALSE, message= FALSE}
# ejemplo tuit con muchos temas

ejemplo2 <- cantidad_temas_tuit  %>%  
  arrange(desc(cantidad_temas_tuit)) %>% 
  head(1)

twitterwidget(as.character(ejemplo2$tweet_id))
```

Este tuit fue asignado a 7 temas: desocupación, obra pública, vivienda, deuda, educación, salud y seguridad. ¿No está tan errado, no?

Veamos, en el agregado de nuestra base, a cuántos temas fue asignado cada tuit.

```{r temas_tuit, warning= FALSE, message= FALSE}
# histograma descriptivo / cantidad de temas por tuit

plot_cantidad_temas_tuit <- cantidad_temas_tuit %>% 
  ggplot(aes(cantidad_temas_tuit)) +
  geom_bar(fill = "#0F92DA") +
  theme_clean() +
  labs(x = "Cantidad de temas por tuit",
       y = "Cantidad de tuits asignados") +
  scale_x_continuous(breaks = seq(0, 7, 1))

plot_cantidad_temas_tuit
```

En el gráfico emerge una de las principales debilidades de este abordaje: un gran número de tuits no fue asignado a ningún tema. Hay margen para perfeccionar el desempeño de estos cálculos en el futuro. 

En cualquier caso, para trabajar en lo que sigue nos quedaremos con los tuits que fueron asignados a un único tema. Hemos pasado de las palabras, a los tuits. Ahora, haremos el salto hacia los emisores, y algunas de las características que los definen. 

```{r unico_tema, warning= FALSE, message= FALSE}
tuits_con_unico_tema <- ncoincidencias_tema_tweets  %>% 
  dplyr::mutate(tweet_id = as.factor(tweet_id)) %>% 
  left_join(cantidad_temas_tuit) %>% 
  filter(cantidad_temas_tuit == 1 )

temas_en_tuits_con_unico_tema <- fct_count(tuits_con_unico_tema$temas)
```

Con esta base, podremos evaluar fácilmente algunas similitudes y diferencias entre los candidatos. Vamos a ensayar algunos ejercicios visuales.

En primer lugar, identificamos cuántos tuits en relación a cada tema emitió cada candidato. Se trata de una cuenta en términos absolutos. 

```{r temas_absolutos, warning= FALSE, message= FALSE}

# temas preferidos por candidato

# cuenta absoluta
# ¿cuantos tuits sobre x tema emitio i candidato?

temas_candidatos_absolutos <- ncoincidencias_tema_tweets %>% 
  dplyr::count(screen_name, temas) %>% 
  na.omit(n) %>% 
  left_join(datos_base)

plot_temas_candidatos_absolutos <- temas_candidatos_absolutos %>% 
  dplyr::mutate(screen_name = as.factor(screen_name)) %>% 
  ggplot(aes(x = fct_reorder(screen_name, Distrito), y = temas, size= n, colour = Distrito)) +
           geom_count() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        legend.position = "none") +
  labs(x = "", y = "")

```

En el gráfico a continuación, la cantidad de tuits emitidos por cada candidato sobre cada tema está dado por el tamaño del círculo :black_circle:.

```{r plot_temas_candidatos_absolutos, warning= FALSE, message= FALSE}
plot_temas_candidatos_absolutos
```

Quizás sea más claro graficar y calcular el peso relativo de cada tema para cada candidato. Lo hacemos a partir del código a continuación.

```{r temas_relativos, warning= FALSE, message= FALSE}
# cuenta relativa
# ¿qué proporción de los tuits emitidos por i candidato fueron a x tema?

# eventualmente podemos hacer esta cuenta más atrás-
# calculamos cantidad de tuits emitidos por candidato
cantidad_tuits_candidato <- joined_candidatos %>% 
  subset( Campaña == 1 ) %>%  
  dplyr::count(screen_name) %>% 
  rename(tuits_emitidos_totales = "n")

# unimos esta base para que la que consigna los temas por candidatos 
# tenga una columna con el total de tuits emitidos

temas_candidatos_relativos <- temas_candidatos_absolutos %>%
  left_join(cantidad_tuits_candidato) %>% 
  dplyr::mutate(n_relativo = n/tuits_emitidos_totales*100)

plot_temas_candidatos_relativos <- temas_candidatos_relativos %>% 
  ggplot(aes(x = fct_reorder(screen_name, Cargo), y = n_relativo, fill = temas)) +
  geom_col(position = "stack") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90)) + 
  theme(legend.position = "bottom",
        legend.key.size = unit(0.4, 'cm'), #change legend key size
        legend.title = element_blank(),
        legend.text = element_text(size=7)) + 
  labs(title = "Proporción de tutis dedicados a cada tema",
       subtitle = "por candidato",
       x = "",
       y = "%")

```

En el gráfico que sigue, chequeamos qué proporción de los tuits emitidos por cada candidato corresponde a cada tema, según nuestras estimaciones. El "espacio en blanco" arriba de las barras indica cuántos tuits de los emitidos por cada candidato no fueron asignados a ninguna temática en particular. 

```{r plot temas relativos, warning= FALSE, message= FALSE}
plot_temas_candidatos_relativos
```

Finalmente, un último ejercicio que puede resultar interesante consiste en identificar, de la lista de términos previamente provista, cuáles fueron las palabras más mencionadas por tema. 

Ensayamos el siguiente gráfico para la temática "vivienda", una de las más mencionadas por nuestros candidatos.

```{r force, warning= FALSE, message= FALSE}

# y de hacer matches por palabras
# hacemos para tema vivienda

matches_vivienda <- temas_palabras_match_tokens_long %>% 
  subset(temas == "vivienda") %>% 
  dplyr::count(screen_name, tokens)

matches_vivienda_paralell <- matches_vivienda  %>% 
  gather_set_data(1:2)

plot_matches_vivienda <- ggplot(matches_vivienda_paralell,
                   aes(x, id = id, split = y, value = n)) + #  INICIA GRAFICO
  geom_parallel_sets(aes(fill = screen_name), alpha = 0.7, axis.width = 0.1, show.legend = FALSE) +
  theme_minimal() +
  geom_parallel_sets_axes(axis.width = 0.1, color = "lightgray", fill = "white") +
  geom_parallel_sets_labels(colour = 'black', angle= 0) +
  theme_no_axes() +
  theme(panel.background = element_rect(fill = "white", colour = "white"))

```

En la representación visual que sigue, observamos algunas diferencias relevantes. Por ejemplo, dentro de este tópico, M. Lammens, candidato porteño, habla más sobre los "alquileres", mientras que candidatos de provincias del interior versan más sobre las "villas". La diferencia puede ser indicativa de la estructura demográfica de cada distirito, y/o de la agenda de políticas en discusión durante la campaña. 

```{r plot force, warning= FALSE, message= FALSE}
plot_matches_vivienda
```

Sería interesante repetir la tarea para otros asuntos. Pero, por ahora, consideramos que hemos exprimido suficientemente esta estrategica. Además, hemos hecho hincapié en sus múltiples debilidades. Ensayemos otra técnica.

## 2: Topic modelling

Dadas las mentadas limitaciones del primer abordaje propuesto, hemos ensayado propuestas menos "supervisadas". 

Nuestro segundo intento por rastrear áreas o asuntos a los que refieren nuestros candidatos consiste en el modelado de tópicos o _topic modelling_, un método para la calsificación no supervisada de "documentos".

Siguiendo la propuesta contenida en ["Text Mining with R. A tidy aproach"](https://www.tidytextmining.com/topicmodeling.html#by-word-assignments-augment), en esta sección aplicaremos el algoritmo LDA (_Latent Dirichlet allocation_) a nuestros datos. 

> _"Latent Dirichlet allocation (LDA) es un método de ajuste de modelos de tópicos particularmente popular. Trata a cada documento como un mix de tópicos, y cada tópico como un mix de palabras. Esto permite que los documentos se "superpongan" en términos de contenido, en lugar de ser separados en grupos discretos (excluyentes), de manera que imita el uso típico del lenguaje natural."_ [^2]

En nuestro caso, trataremos a cada tuit individual como un "documento". [En este script](https://github.com/CVFH/Tuits_arg_2019/blob/master/exploracion_discursos.R) experimentamos con diversas alternativas para el cómputo del algoritmo seleccionado. Aquí exponemos la opción que hallamos más convincente.

Esta consta de dos decisiones de importancia. 

1. Una vez más, para las estimaciones de nuestro interés, requerimos descomponer a cada tuit en sus términos constitutivos. En esta oportunidad hemos escogido separar a los tuits en "bigramas", unidades de dos palabras. 
2. Debimos escoger la cantidad de tópicos que suponemos que subyacen a nuestros datos (el parámetro "k" en el código de abajo). Tras varios intentos, consideramos que con 8 tópicos obteníamos algunos resultados interesantes. 

Nuestras elecciones han sido abiertamente arbitrarias. A futuro, otras implementaciones podrán proceder de manera más criteriosa o teóricamente fundada. 

Exponemos enseguida el código utilizado para el análisis:

```{r topics calculos, warning= FALSE, message= FALSE}
# Topic modeling ######
# con 8 temas y bigramas

# preparando datos
# LDA trabaja con estructura de matriz 

candidatos_tokens_dtm <- joined_candidatos %>% 
  tokenizarTextoTuits(tipo_token = "ngrams") %>% 
  limpiarTokens(bigramas = TRUE, palabras_web = TRUE, hashtags = TRUE, mentions = TRUE) %>% 
  subset(!str_detect(tokens, "NA NA")) %>% 
  dplyr::count(tweet_id, tokens) %>%
  cast_dtm(tweet_id, tokens, n)

# aplicamos algoritmo

candidatos_tokens_LDA <- LDA(candidatos_tokens_dtm, k = 8, control = list(seed = 1234))
```

Tras estas líneas, ya podemos experimentar con los resutlados obtenidos.

La primera tarea consiste en intentar categorizar qué temas ha identificado nuestro algoritmo.

Para ello, contrastamos las "beta" asignadas a cada bigrama: la medida en que cada par de términos está asociada a uno de los 8 "temas". Viendo cuáles son los bigramas más fuertemente vinculados a un tema, podemos tratar de deducir de qué va el asunto.

```{r topics palabras, warning= FALSE, message= FALSE}
#per-topic-per-word probabilities

candidatos_tokens_topics <- tidy(candidatos_tokens_LDA, matrix = "beta")

# nos quedamos con los términos principales

candidatos_tokens_topics_top_terms <- candidatos_tokens_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) 

# graficamos

plot_topics_terminos <- candidatos_tokens_topics_top_terms  %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_y_reordered() +
  theme_clean() +
  theme(strip.text = element_text(size = 7),
        axis.text.y = element_text(size = 7, hjust = 0.8),
        axis.text.x = element_blank())  +
  labs(title = "Términos por temas",
       subtitle = "Palabras más probablemente pertenecientes a un mismo tópico",
       x = "beta",
       y = "")

plot_topics_terminos
```

Nuestros resultados no son grandiosos, pero nos permiten alguna que otra reflexión interesante. Por ejemplo, la educación parece ser un tema en sí mismo, de acuerdo con este algoritmo (5); parecería que también el "desarrollo" (6) y la seguridad social en un sentido amplio (4)y, más genéricamente, "el futuro" (2). 

De todos modos, muchas palabras son o muy poco o demasiado específicas. Destacan sobre todo los nombres propios de lugares. Pueden estar ocurriendo dos cosas: o bien los "temas" hacen a la idiosincracia de cada lugar, o bien esta última enmascara otras similitudes.

Futuras estimaciones podrán perfeccionar la aplicación de este algoritmo. De todos modos, juguemos un poco más de momento. 

Una segunda tarea consiste en ver la probabilidad de que cada documento (recuérdese: cada tuit) pertenezca a cada uno de los 8 temas. Esto indica el coeficiente "gamma". 

Cada documento posee 8 "gammas", uno para cada tema. Nosotros nos quedamos con el mayor de ellos, suponiendo que indica el tema al que pertenece el tuit en cuestión.

```{r topics gamma, warning= FALSE, message= FALSE}
# per-document-per-topic probabilities,  γ (“gamma”).

candidatos_tuits_topics <- tidy(candidatos_tokens_LDA, matrix = "gamma") %>% 
  dplyr::rename(tweet_id = "document") %>% 
  left_join(joined_candidatos %>% dplyr::mutate(tweet_id = as.character(tweet_id) )) %>% 
  select(tweet_id, topic, gamma, screen_name, fav_count, rts, created_at)

# nos quedamos con el mayor gamma

candidatos_tuits_topics_top <- candidatos_tuits_topics %>% 
  group_by(tweet_id) %>% 
  dplyr::summarise(gamma = max(gamma)) %>% 
  left_join(candidatos_tuits_topics) %>% 
  left_join(datos_base)
```

Ahora tenemos una base cuyas observaciones son tuits, y cada uno está asociado con cierto grado de probabilidad (gamma) a un tema. Con esto, podemos visualizar de manera agregada nuestros datos. 

Veamos primero cuántos tuits por tema emitió cada candidato. Emergen algunas diferencias. Por ejemplo, A. Rodríguez Saá refiere mucho a los temas (1) y (2); A. Bonfatti, al (6); N. del Caño, al (5); C. Poggi al (5) y al (7). Nos excede extendernos en la interpretación de estas diferencias, cuestión acerca de la que no descartamos explayarnos en el futuro. 

```{r topics plot sname, warning= FALSE, message= FALSE}
#graficando.

#agrupados por  screen name 
candidatos_tuits_topics_top_grouped_sname <- candidatos_tuits_topics_top %>% 
  dplyr::count(screen_name, topic)

plot_topics_candidatos  <- candidatos_tuits_topics_top_grouped_sname %>% 
  ggplot(aes(topic, n, colour= screen_name)) +
  geom_point()  +
  facet_wrap(~ screen_name, ncol = 3) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title= "Temas preferidos por los candidatos",
       subtitle = "Medidos a partir de cantidad de tuits emitidos por candidato que probablemente están asociados a un tema",
       x = "Cantidad de tuits",
       "Tópicos")

plot_topics_candidatos 
```

Otra posibilidad consiste en inspeccionar ocularmente cuántos tuits por tema fueron emitidos por los candidatos a Gobernador versus los candidatos a la Presidencia. Para este cálculo utilizamos una medida relativa: qué proporción del total de tutis emitidos por los candidatos a uno y otro cargo corresponden a cada uno de los 8 temas.

```{r topics plot cargo, warning= FALSE, message= FALSE}
#agrupados por cargo 

# calculamos proporcionalmente
tutis_totales_cargo <- joined_candidatos %>% 
  filter(Campaña==1) %>% 
  dplyr::count(Cargo) %>% 
  dplyr::rename(tuits_totales_cargo = "n")

candidatos_tuits_topics_top_grouped_cargo <- candidatos_tuits_topics_top %>% 
  dplyr::count(Cargo, topic) %>% 
    left_join(tutis_totales_cargo) %>% 
    dplyr::mutate(proporcion_tuits_topic_cargo = n/tuits_totales_cargo *100)

plot_topics_cargo <- candidatos_tuits_topics_top_grouped_cargo %>% 
  ggplot(aes(topic, proporcion_tuits_topic_cargo, colour= Cargo)) +
  geom_point(size = 3) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 8, 1)) +
  labs(title= "Temas preferidos por los candidatos",
       subtitle = "según la cartera a la que compiten",
       x = "Tópicos",
       y = "Proporción de tuits",
       colour = "") +
  theme(legend.position = "bottom")

plot_topics_cargo
```

Hay algunos hallazgos interesantes. Mientras que en el tema (6), que decíamos parecía designar al "desarrollo", no hay diferencias en la proporción de tuits que a este asunto destinan los candidatos a una u otra cartera, en otros se registra una atención dispar. Por ejemplo, los candidatos a Gobernador refieren más asiduamente que los candidatos a la Presidencia al tema (8). Inversamente, estos últimos se explayan proporcionalmente más en torno al tópico (3). De todos modos, las diferencias no son sustantivamente enormes. 

Repliquemos este ejercicio pero clasificando, ahora, a los tuits conforme al timing de la campaña (es decir, a si fueron emitidos durante una campaña "desdoblada" o "simultánea").

```{r topics plot tfecha, warning= FALSE, message= FALSE}
#agrupados por tipo_fecha 

tutis_totales_tfecha <- joined_candidatos %>% 
  filter(Campaña==1) %>% 
  dplyr::count(tipo_fecha) %>% 
  dplyr::rename(tuits_totales_tfecha = "n")

candidatos_tuits_topics_top_grouped_tfecha <- candidatos_tuits_topics_top %>% 
  dplyr::count(tipo_fecha, topic) %>% 
  left_join(tutis_totales_tfecha) %>% 
  dplyr::mutate(proporcion_tuits_topic_tfecha = n/tuits_totales_tfecha*100 )

plot_topics_tfecha <- candidatos_tuits_topics_top_grouped_tfecha %>% 
  ggplot(aes(topic, proporcion_tuits_topic_tfecha, colour= tipo_fecha)) +
  geom_point(size = 3) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 8, 1)) +
  labs(title= "Temas preferidos por los candidatos",
       subtitle = "según el calendario electoral",
       x = "Tópicos",
       y = "Proporción de tuits",
       colour = "") +
  theme(legend.position = "bottom")

plot_topics_tfecha
``` 

Aquí hay muchos tópicos que parecen consignar una atención proporcionalmente homogénea: sobre todo, el (2) y el (4); en menor medida, el (1), el (5) y el (6). Sí son mas notorias las diferencias entre los asuntos (3), más referido en las elecciones "simutláneas" y (7), más atendido en las desdobladas.

La técnica de _topic modelling_ parece prometedora, pero nuestra aplicación claramente deja bastante que desear. Esperamos a futuro refinar nuestro manejo de este algoritmo, a los fines de identificar de manera menos arbitraria los "temas" sobre los que versan los tuits de nuestros candidatos.

## 3 : Análisis (fallido?) de componentes principales

Una última estrategia que quisimos poner a prueba en la búsqueda de una respuesta a nuestras interrogantes consistió en el _análisis de componentes principales_ (PCA por su sigla en inglés). La técnica ya ha sido empleada en la clasificación de documentos de texto en general, y de tuits en particular. ¿Por qué no inspeccionar nuestra muestra bajo esta lupa?

Nuestros conocimientos en la materia son más que incipientes, pero, apoyándonos en trabajos previos[^3], nos las arreglamos para aplicar el comando `prcomp()` a nuestros datos e interpretar los resultados. Sin embargo, estos nos resultaron decepcionantes. Desconocemos si "somos nosotros o son ellos": si la técnica resulta inadecuada a la estructura de nuestros datos, o si nuestra estimación ha sido poco sofisticada. 

En cualquier caso, como dicen que de los errores también se aprende, comentamos brevemente a nuestro lector las operaciones realizadas.

En esta oportunidad no debimos incorporar ninguna medida _a priori_ de cuáles o cuántos temas ocupaban a los tuiteos de nuestros candidatos. De manera similar al _topic modelling_, el _PCA_ trabja sobre una matriz de documentos / términos, pero el cálculo que hicimos carecía de restricciones en términos de la "cantidad de temas" a buscar (`k` en el comando `LDA`). 

Sí nos enfrentamos, nuevamente, a la disyuntiva: ¿cuáles son nuestros "documentos"? ¿cada tuit o el discurso de los candidatos en Twitter tomado en su globalidad?

Ensayamos estimaciones con ambas alternativas. 

Consideremos primero al discurso de cada emisor como un único documento. 
He aquí el código empleado, del que se sigue el gráfico a continuación:

```{r pca1, warning= FALSE, message= FALSE}
# preparamos datos en tokens tidy
candidatos_tokentweets  <- joined_candidatos %>% 
tokenizarTextoTuits(tipo_token = "tweets") %>%  
left_join(datos_base)

# preparamos matriz de datos 
candidatos_tokentweet_dtm <- candidatos_tokentweets %>% 
  limpiarTokens() %>% 
  dplyr::count(screen_name, tokens) %>% 
  cast_dtm(screen_name, tokens, n)

# PCA

# con este comando extrajimos los resultados:
pca_res <- prcomp(candidatos_tokentweet_dtm, scale=TRUE)

# diversas medidas de resumen (no mostramos aquí)
# summary(pca_res)
# pca_res$x[1:5,1:3]
# pca_res$center[1:5]
# head(pca_res$scale^2, n=5)

# cálculo de varianza explicada por cada componente

var_explained <- pca_res$sdev^2/sum(pca_res$sdev^2)
# var_explained[1:5]

# ploteamos resultados
pca_res$x %>% 
  as.data.frame %>%
  rownames_to_column("screen_name") %>%
  ggplot(aes(x = PC1,y = PC2)) +  
  # nos quedamos con los primeros dos componentes principales
  geom_point(aes(color=screen_name),size = 4) +
  geom_text(aes(label=screen_name)) +
  labs(x=paste0("PC1: ",round(var_explained[1]*100,1),"%"),
       y=paste0("PC2: ",round(var_explained[2]*100,1),"%")) +
  theme_clean() + 
  theme(legend.position="none") 

```

El gráfico muestra la posición de nuestros documentos (los candidatos) respecto de los dos primeros componentes que resultaron de las estimaciones. ¿Qué ha ocurrido? Sencillamente, cada candidato en sí mismo explica un componente. Es decir, la variabilidad en nuestros datos estaría explicada por la idiosincrasia de cada uno de los documentos imputados en el cálculo. 

Veamos ahora qué ocurre si calificamos a cada tuit único como un documento único. 
Para proceder, haríamos algo como el código que sigue: 

```{r pca2, eval=FALSE, warning= FALSE, message= FALSE}
candidatos_tokentweet_dtmv2 <- candidatos_tokentweets %>% 
  limpiarTokens() %>% 
  count(tweet_id, tokens) %>% 
  cast_dtm(tweet_id, tokens, n)

# PCA
pca_resv2 <- prcomp(candidatos_tokentweet_dtmv2, scale=TRUE)
```

Pero... no lo hacemos: no pudimos procesar semejante matriz de datos :fire:. 
Intentamos varias estrategias de limpieza y reducción de la complejidad de nuestra base. En el "mejor" de los casos (¡la PC no se rompió! :trollface:), los resultados sugirieron que nuestros datos estaban organizados en torno a más de 300 componentes, que explicaban menos de un 1% de varianza cada uno.
Claramente, el cálculo no es muy informativo acerca de si y cuáles "variables subyacentes" explican la variabilidad en nuestros datos.

---

Hemos transitado un largo camino: partiendo de simples pero abundantes piezas de datos y de funciones, hemos tratado de asir qué tan populares y elocuentes, y qué relaciones y temas abordaron los candidatos a las elecciones de 2019 en la Argentina. Hemos intentado mantener un equilibrio entre la exposición del código y el relato de nuestras operaciones, poniendo en relieve algunos de los resultados alcanzados. Sin dudas, queda mucho por trabajar. Las últimas líneas de este último post son claro ejemplo de ello. Agradecemos la compañía de nuestro lector :clap:, y le invitamos a que haga críticas y devoluciones :pencil2: que nos perimtan seguir construyendo sobre este trabajo :raised_hands:.

Volver al [inicio :house:](/)

[^1]: Nótese que la base, definida previamente, contiene términos que serán inutilizados, ya que trabajaremos con palabras únicas. 
[^2]: [La cita proviene del libro referido](https://www.tidytextmining.com/topicmodeling.html#by-word-assignments-augment). La traducción es propia.
[^3]: [Esta entrada](https://cmdlinetips.com/2019/04/introduction-to-pca-with-r-using-prcomp/) fue particularmente útil, también agradecemos al [Tuq](https://www.tuqmano.com/) por su asesoramiento. 