---
title: Preparación de Datos
author: Carolina Franco
date: '2021-01-27'
slug: preparacion_datos
tags: 
  [preparacion, datos]
weight: 2
---

En este post mostraremos los datos que utilizamos, e introduciremos dos subsecciones que permiten seguir los pasos que hicimos para obtenerlos.

Antes de comenzar, abrimos algunas librerías.

```{r, message=FALSE, warning=FALSE}
library(tidyverse) # trabajaremos en el paradigma "tidy"

source("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/Modules/funcionesGraficos.R", encoding = "UTF-8") # script propio, para formatear tablas
```


## Nuestra base

Comencemos por presentar sintéticamente los datos elejidos para trabajar. 
Como dijimos, nos interesaba explorar el desempeño y los discursos de los candidatos al poder ejecutivo en las elecciones argentinas de 2019 -un país federal, que incluye cargos ejecutivos electivos a diversos niveles. 
En este marco, y dado nuestro fin experimental y exploratorio, decidimos quedarnos con los siguientes datos: 

1. De los candidatos a la Presidencia (6 candidatos en total)
2. De candidatos a Gobernador conforme al siguiente recorte:
    + Seleccionamos 4 provincias que oficiaron elecciones "simultáneas" (al mismo tiempo que en el nivel nacional), y 4 provincias que realizaron elecciones "desdobladas", (antes que en el nivel nacional) pero en una única fecha. 
    + Nos quedamos con los candidatos que llegaron al primer y segundo puesto. 
    
Nuestra base quedó conformada de la siguiente manera:

```{r, message=FALSE, warning=FALSE}
# Llamamos al script que agrega y procesa datos. 
# Nos extenderemos sobre este punto enseguida

source("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/preparacion_datos_tuits.R", encoding = "UTF-8")

# Solicitamos datos

datos_base <- traerDatosTuits("base")

# visualizamos 

datos_base %>% formatoTabla(
        titulo = "Datos Utilizados",
      subtitulo = "TuitsArg2019")
```


## Datos Electorales

Para "explorar la popularidad" de los candidatos (como veremos en la [sección correspondiente](../explorando_popularidad/)), debimos colectar datos sobre los resultados electorales. 

Para eso desarrollamos [este script](https://github.com/CVFH/Tuits_arg_2019/blob/master/preparacion_datos_electorales.R) que explicamos en [este post](../../post/preparacion_electorales/). El código define una función que parte de enlaces de Wikipedia y/o de la Dirección Nacional Electoral argentina, y devuelve una tabla de datos _tidy_[^1].

Por ejemplo, partimos del enlace: 
[https://es.wikipedia.org/wiki/Elecciones_presidenciales_de_Argentina_de_2019](https://es.wikipedia.org/wiki/Elecciones_presidenciales_de_Argentina_de_2019)

...que contiene la siguiente tabla:

![](tabla.JPG)

Extrajimos de esta manera los datos:

```{r, message=FALSE, warning=FALSE}

# Primero, llamamos al script que diseñamos

source("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/preparacion_datos_electorales.R", encoding = "UTF-8")

# Llamamos a una función para obtener los datos presidenciales, 
# asignando el parámetro correspondiente ("presid")

votos_presid <- traerDatosElectorales("presid")

# Tenemos nuestros datos!

votos_presid %>% formatoTabla(
        titulo = "Elecciones presidenciales -  Argentina 2019",
      subtitulo = "Resultados")

```


## Datos de tuiter

Una vez identificadas las cuentas de nuestro interés, nos ocupamos de traer los datos de los tuits emitidos con _rtweet_. 

Por ejemplo, para @[Kicillofok](https://twitter.com/Kicillofok), utilizamos el comando:

`tuits_Kicillofok <- get_timeline("Kicillofok")`

Con esto, obtenemos una tabla (aprox) como la siguiente: 

```{r, eval = TRUE, echo = FALSE, message=FALSE, warning=FALSE}
tuits_Kicillofok <- read_csv("https://raw.githubusercontent.com/CVFH/Tuits_arg_2019/master/Data/Kicillofok.csv")
```

```{r, message=FALSE, warning=FALSE}

head(tuits_Kicillofok, 6) %>% formatoTabla(
        titulo = "Base cruda",
      subtitulo = "muestra de ejemplo",
      back_color = "#C0EEFF")

```

El default de `get_timeline` trae los últimos 100 tuits. Como máximo, las normas de Twitter permiten extraer 3.200 tuits. Dado que este límite implica que la fecha de los tuits que obtenemos con el llamado va cambiando con el tiempo, nosotros optamos por guardar estas bases de datos en archivos .csv, a los que se puede acceder desde [este enlace a nuestro repositorio de Git](https://github.com/CVFH/Tuits_arg_2019/tree/master/Data).

Por este mismo motivo, desarrollamos una función que agrega y emprolija estos datos, de manera similar a como hicimos con los datos electorales. Pueden verla en [este script](https://github.com/CVFH/Tuits_arg_2019/blob/master/preparacion_datos_tuits.R), que explicamos en [este post](../preparacion_tuits/).

El resultado de aplicar estas operaciones es una tabla como la siguiente:

```{r muestra_datos_tuits, warning=FALSE, message=FALSE}
joined_candidatos <- traerDatosTuits("tot") %>% 
  arrange(desc(rts))

# dimensiones de esta base

dim(joined_candidatos)

# pequeña muestra

head(joined_candidatos, 6) %>% formatoTabla(
        titulo = "Base transformada",
      subtitulo = "muestra de ejemplo",
      back_color = "#C0EEFF")
```

## Comentarios adicionales: desafíos y límites de la muestra seleccionada

Quizás convenga, antes de continuar, hacer un par de aclaraciones respecto de los datos con los que trabajamos.

En primer lugar, vale explicitar una importante decisión. Para responder a nuestras preguntas, debimos identificar los tutis emitidos por los candidatos _durante la campaña_. Siendo que la red social permite un flujo permanente de una multiplicidad de mensajes (personales, laborales, etc), aquí es justo preguntarnos: ¿cuáles de ellos hacen a dicha "campaña"?

La alternativa que hallamos menos ambigua fue recortar nuestra base de datos conforme a la fecha de emisión del tuit: pocas dudas caben de que la campaña _termina_ el día de los comicios. Pero todavía pende la pregunta: ¿cuándo empieza?

Aquí nos basamos en la Ley Electoral nacional para ofrecer, de nuevo, la respuesta que consideramos menos arbitraria. En el país se realizan elecciones "primarias abiertas, simultáneas y obligatorias", "PASO". Sólo entonces se define con seguirdad "la cancha" en la que se jugará la partida política de cara a las elecciones generales. 

El período comprendido entre estas dos fechas, entonces: las PASO y las elecciones, podría ser justamente definido como de campaña.

Ahora bien, la estructura federal de la Argentina nos impuso un desafío adicional: recordemos que las provincias tienen margen para ajustar el calendario electoral a sus preferencias. Para complicar aún más las cosas, no todas las provincias llevan a cabo elecciones primarias. 

Como dijimos más atrás, nuestros datos deliberadamente contenían tuits emitidos por los candidatos a la cartera ejecutiva nacional, y a 4 provincias que oficiaron elecciones simultáneas, y 4 que lo hicieron de manera "desdoblada", pero en una misma fecha. 

Nuestra solución entonces fue la siguiente. Como fin de la campaña, consideramos la fecha de las respectivas votaciones: 16 de junio para las cuatro provincias que desdoblaron y adelantaron el día, 27 de octubre para la nación y el resto de las provincias. Y, si bien no todas las provincias celebraron primarias, como inicio consideramos la fecha en la que se realizaran en el resto de los distritos del mismo "grupo": 28 de abril para las provincias que modificaron su calendario respecto al nacional, con base en las primarias de Santa Fe, 11 de agosto para el resto, de los cuales sólo La Rioja no llevó adelante esta primera instancia electoral. 

Sintetizamos el resultado de este recorte en la tabla a continuación:

```{r muestrafechas, warning=FALSE, message=FALSE}
tabla_fechas <- read_csv("muestra_fechas.csv")

tabla_fechas %>%  formatoTabla(
        titulo = "Recorte de 'la campaña'",
      subtitulo = "muestra resultante por provincia")
  
```

Finalizamos con una última pero no menos importante aclaración. Como hemos dicho unas líneas más atrás, _Twitter_ tiene un límite para la descarga de datos de la _línea del tiempo_ de cada usuario: 3.200 tuits. Esto constriñe "qué tan atrás" podemos recabar información. Lamentablemente, no hemos obtenido los datos que hubiéramos querido para todos los candidatos. En nuestro análisis, hemos hecho la vista gorda a semejante falta. Una investigación más sería deberá (o debería) considerar un tratamiento más sistemático de estos _missings_. 

Para información del lector, anotamos aquí el primer y último tuit que hemos logrado asir para cada candidato. Agregamos una columna que indica la cantidad de tuits totales disponibles.

```{r muestra_joined, warning=FALSE, message=FALSE}

joined_candidatos <- joined_candidatos %>% 
  left_join(datos_base)

fechas_tuits <- joined_candidatos %>% 
  group_by(screen_name) %>% 
  summarize(ultimo = max(as.Date(created_at)),
            primero = min(as.Date(created_at)),
            cantidad_tuits = n())

fechas_tuits %>% formatoTabla(
        titulo = "Recorte temporal de la base de datos de tuits",
      subtitulo = "por candidato")
  

```

##### Más?

Si querés interiorizarte mejor con la manera en que manipulamos los datos, te recomendamos la lectura del [post que explica la función para formatear datos electorales](../../post/preparacion_electorales/), y [el que hace lo propio con los datos de Twitter](../preparacion_tuits/).

## A trabajar!

Una vez obtenidas nuestras tablas electorales y los datos de Twitter, podemos jugar, explorar, ensayar un análisis, algo que haremos en los posts que siguen. Recomendamos el camino de lectura propuesto en [el inicio de este blog :house:](/).


[^1]: En el proyecto original, trabajamos con versiones locales de todas estas piezas de código, que, por motivos de espacio y "portabilidad", decidimos consignar en archivos separados. Para su exposición en este post, y los que siguen en el camino propuesto, hemos transitado a su versión "online", es decir, convocamos directamente al código almacenado en [nuestro repositorio de Git](https://github.com/CVFH/Tuits_arg_2019). El fin es que lo expuesto sea reproducible por quien así lo desee. Sin embargo, nos ha significado una importante pérdida en términos de agilidad de procesamiento. Pretendemos experimentar a futuro con alternativas más "económicas".